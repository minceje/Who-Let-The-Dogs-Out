\documentclass[12pt]{report}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epsf}
\usepackage{amsrefs}




\title{Deep Learning Image Recognition: 
	Identifying Predominant Breeds within Mixed-Breed Dogs}
\date{November 23rd, 2018}
\author{Julia Goyco, Zoe Lambert, Jennifer Mince, Stephanie Schoch}
\begin{document}
	\maketitle
\section*{Abstract} 
\indent	\par  This paper details our use of a convolutional neural network (CNN) model to predict the two predominant dog breeds from an image of a mixed-breed dog. Our first dataset consists of images of purebred dogs from Kaggle. Our second dataset consists of images of mixed dog breeds from 101 dogs.  We selected a pre-trained CNN model, which was trained on ImageNet. Then we performed two steps of transfer learning. We re-trained the model with the first dataset and retrained again with the second dataset.
		\par 
\newpage
 \section* {Introduction} 
		
\indent	\par The purpose of this paper and project was image recognition of dog breeds through deep learning methods. The guiding question that led to this research was: can we use a deep learning approach to determine the two predominant breeds in a mixed-breed dog? 
\par The project began by looking at current dog breed image recognition projects for a starting point to work off of and change to fit the purpose of recognizing the two dog breeds that created the mixed-breed fed into the model.
\par Deep learning involves learning from data without a human explicitly programming the rules [A]. There are several types of deep learning networks that have been developed, each with different strengths and weaknesses in terms of applications. Two primary networks include convolutional neural networks and recurrent neural networks.
\par A convolutional neural network is a type of feedforward neural network with a convolutional layer at its core [A]. The convolutional layer has parameters in the form of learnable filters with small receptive fields extending through the “full depth of the input volume,” which are then connected to one or more fully connected layers [A]. Additionally, there is a fully convolutional network in which the fully connected layers are all replaced with convolutional layers [A]. In contrast, a recurrent neural network (RNN) can have activations that flow in a loop as connections form directed cycles [A]. RNNs are different from CNNs in that they can process “arbitrary sequences of inputs” by using internal memory [A]. Finally, a special type of RNN is an LSTM (long-short term memory) neural network which can “remember a value for an arbitrary length of time. Convolutional neural networks have had remarkable success in the area of computer vision, outperforming other neural networks on similar image recognition and image classification tasks [B]. One specific type of image classification that remains a challenge, however, is fine-grained image classification, which is relevant to the challenge of dog breed identification.
\par The challenge of dog breed identification deals with fine-grained image classification, which “aims to distinguish subordinate-level categories under some basic-level category” [A]. This is a particularly challenging task for neural networks due to “high intra-class and low interclass variance” [A], in other words, there can be significant variations within individual classes and very slight variation between classes so classes can only be categorized based on subtle, local differences [E]. Further, the high intra-class variance can occur due to changes in the foreground or variation in pose, scale, or rotation, that make identifying discernable differences between classes difficult as the discriminative features are localized in object parts, such in dog faces [E]. To solve this issue, a common approach to fine-grained image classification is to pipeline the process by first determining where the object is by determining what the foreground of the image is and then extracting discriminative features from the target object [E].

\section* {Current State of the Art} 
		
\indent	\par There have been several recent developments and projects on fine-grained image classification. Xiao et al. (2015) applied visual attention to domain-specific fine-grained image classification by using three types of attention: bottom-up, object-level top-down, and part-level top-down, tested on two datasets: ILSVRC2012 dataset and CUB200_2011 dataset [E]. In detail, bottom-up proposes patches to attenuate on, object-level top-down selects patches of an object, and part-level top-down localizes features of the object [E]. They trained and tested on fine-grained image classification of dogs and birds in the datasets.
\par The use of both the bottom-up and top-down approaches helps to identify the “where” and the “what” of the image, respectively. First, the bottom-up approach helps to identify patches in an image that may contain parts of a specific object [E]. This leads to high recall but low precision, so the top-down approach supplements it and helps determine discriminative features that can then be used to classify an object [E], for example, a dog’s tail or ears. In other words, the bottom-up and top-down help with object-level and part-level discrimination [E]. At the object-level, the goal is to remove noisy patches not relevant to the object, and at the part-level, the goal is to detect and cluster patches to determine local features that can be used for classification [E]. Further, with this approach, the researchers only used image-level labels because providing detailed labeling with bounded boxes is an expensive process in terms of time [E]. The architecture was a convolutional neural network with 5 convolutional layers and 3 fully connected layers, with the number of neurons on the output layer equal to the number of categories [E].  Since this model uses feature extraction, the first fully-connected layer of the CNN outputs features [E]. The researchers found increased performance on the CUB200-2011 dataset under the weakest supervision setting (class labels only) [E]. They partially attribute this to the part-level attention that focuses on local patterns, as this helps deal with shift/scale variances and helps achieve pose normalization for the object [E]. This points to a challenge of fine-grained image classification, the challenge of possibly small datasets. Since we will be working with a dataset we develop of mixed dog breeds, we realize the need for pose normalization as well as accounting for variance in factors such as dog age or variations of markings. For this reason, we began exploring pre-trained models that were trained on large datasets of images, the ImageNet dataset in particular, that could then be used for transfer learning.
\par One of the prominent pre-trained model developments was that of the Inception architecture, introduced by Szegedy et al. in 2014 [B]. Prior to Inception, the primary architecture for fine-grained image classification was the VGG architecture which used a stack of simple convolution layers [B]. Convolution layers operate by learning filters for 3 dimensions using two spatial dimensions, specifically width and height [B]. Further, convolution kernels must “simultaneously map cross-channel correlations and spatial correlations,” which is one area in particular on which Inception improved. Inception introduced a new architecture by using stacked modules, rather than simple convolution layers, which allows the Inception architecture to learn “richer representations with less parameters” [B]. Specifically, these modules look at cross-channel correlations via a 1x1 convolution, then map input onto several smaller spaces and finally maps all correlations on the smaller 3D spaces via 3x3 or 5x5 convolutions [B]. Thus, the module architecture of Inception allows cross-channel correlation and spatial correlation to be decoupled so they do not have to be mapped jointly, which leads to increased performance over the VGG architecture [B].
\par However, in 2017 Francois Chollet of Google proposed a new architecture that would improve upon the Inception architecture. developed a model called Xception, or “Extreme Inception” [B]. Chollet proposed that a 1x1 convolution could be used to map cross-channel correlation, then the spatial-correlation of every output channel could be separately mapped, further decoupling spatial and cross-channel correlation [B]. This is similar to depthwise separable convolution, in which a spatial convolution is “performed over each channel of an input, followed by a pointwise convolution,” such as a 1x1 convolution, which projects output from the channels onto a new channel space [B]. This was introduced in 2014 and was included in the TensorFlow framework in 2016 [B].  The difference, however, between the depthwise separable convolution and the Extreme Inception proposed by Chollet, is that the depthwise separable convolution performs the channel-wise spatial convolution first, then the 1x1 convolution [B]. So, essentially, Chollet suggested replacing the modules of the Inception architecture with depthwise separable convolutions by using the depthwise separable convolution implementation available in Tensorflow [B]. By using a convolutional neural network architecture based on depthwise-separable convolution layers, the mapping of cross-channel correlations and spatial correlations in feature maps could be entirely decoupled [B].
\par The Xception architecture uses 36 depthwise separable convolution layers convolution layers structured as 14 modules for feature extraction, which forms the base of the network [B]. All layers except the first and last modules have linear residual connections, and the modules are followed by a logistic regression layer [B].  The architecture has similar parameter count as Inception V3, but does exhibits linearity in the linear stacks of layers, so it exhibits faster convergence and better final performance [B]. This shows that the architecture’s enhanced performance is a result of efficient use of parameters, rather than increased capacity [B]. To demonstrate this, Chollet found small gains in classification performance on the ImageNet dataset over Inception V3 [B]. Due to the efficiency of Xception, we chose to use this as the basis for applying transfer learning to mixed-dog-breed classification [B].	

		
\section* {Implementation} 

\indent	\par A detailed summary of your implementation and justification of your choices in your model:
After looking at the work already established on dog breed image recognition, the group began examining and working with the Stanford Dog Breed Classification project (Stanford University).  The dataset consisted of 20,580 labeled images of dogs, each belonging to one of 120 dog breeds. As the team started to use the stanford dataset we notice the labels seemed be off. 
\par The team decided to use a dataset from kaggle. The dataset from kaggle contains the same images as the stanford dataset but it is split differently it is about 50/50. This dataset also had a labels.csv, this was very useful. This gave the team an example of how to create labels for the mixed breed images. 
\par The mixed dog breed dataset was created by saving images from 101 dog breed. From there, it was imperative to find images of mixed-breed dogs. Importantly, the team noted that only choosing dogs with two distinct parent breeds would make it easiest for the model to learn and accurately identify. Another important thing to note is that the dog breeds had to be a mix of the dogs already included in the training data being used from the previously created Stanford data set. The team split up responsibilities to find dog breeds that fit this description and had images readily available for use. The site “101 Dog Breeds” had a fairly good amount of mixed-breeds that came from two distinctly different breeds with features that related back to each of the parent breeds (101 Dog Breeds). Each breed can be search and includes details about the breed and a gallery of images with different dogs of various ages, sizes, colors, per breed. These images were saved in a similar way to the Stanford Dog Breed data set so they could be uploaded to the code and model easily. The specific breeds chosen to start with were: the Golden Saint (Golden Retriever and Saint Bernard), the Cheagle (Chihuahua and Beagle), the Peekapoo (Pekingese and Poodle), the Pitsky (Pitbull and Husky), the Siberian Retriever (Siberian Husky and Golden Retriever), the Puggle (Pug and Beagle), the Maltipom (Maltese and Pomeranian), the Schnoodle (Schnauzer and Poodle), the Chug (Pug and Chihuahua), the Meagle (Miniature Pinscher and Beagle), the Dorkie (Dachshund and Yorkshire Terrier), the Akita Chow (Akita and Chow Chow), the Bernedoodle (Bernese Mountain Dog and Poodle), the Labrahuahua (Labrador Retriever and Chihuahua), the Mal Shi (Maltese and Shih Tzu), the Basset Shepherd (Basset Hound and German Shepherd), the Beagle Bull (Beagle and Pitbull), the Border Beagle (Border Collie and Beagle), the Shepherd Pit (German Shepherd and Pitbull), the Chorkie (Chihuahua and Yorkshire Terrier), the Havapoo (Havanese and Poodle), and the Bull Pug (Pitbull and Pug).		

\section* {Data and Processing} 
		
\indent	\par  
\par The Kaggle data set contained train images along with a labels.csv. The labels.csv contained the dog breed labels for the train images. The labels.csv had contained 2 columns: id and breed. The ID matched the name of the image and the breed was the pure breed dog that was shown in the image. The labels file and train images were passed into our “Kaggle Dataset Preprocessing” notebook file. The code then sent the images into 4D tensor array along with the correct labels and saved as separate image and label files. These new files were then used in the model,split into 20% test and 80% train, and then set up to see if it could then determine the breed of the dog in a given image.
\par The mixed-breed data images were split 80-20 into mostly training so that the model would learn more from the images to better predict the ones it was tested on. The images were manually split into two folders: train and test. Then Excel label files were created for each folder with two columns: ID and Breed. The ID matched the name of the image and the breed was the mixed-breed that was shown in the images. The label files and images were passed into our “Processing Mixed Breed Data” notebook file. The code then sent the images into arrays along with the correct labels and saved as separate image and label files. These new files were then used in the model to see if it could then determine which two parent breeds made up the mixed-breed image. 

\section* {Model Description}
\indent \par
\par The team chose a Convolutional Neural Network (CNN) for the preliminary model because research showed most projects of similar type chose this option, such as,... (explain and cite). The option to change to a Recurrent Neural Network (RNN) was present in case the CNN could not fulfill what was needed from the model to correctly identify mixed-breeds. A RNN would have the same problem with the amount of time it would take to train. The team ultimately choose to use a transfer learning technique.
\par The team decided to implement transfer learning technique to reduce the amount of time to train a Convolutional Neural Network. Transfer learning is a technique where a pre-trained model is re-purposed for a different task. The team chose to use the keras Xception model. This model is for image classification with weights trained on ImageNet. On ImageNet this model gets an accuracy of 94.5%. [https://keras.io/applications/#xception]
\par On top of the pre-trained Xception model some addition layers were added to increase performance. The additional layers include a global average pooling layer, a fully connected layer and a dropout layer.[ https://www.depends-on-the-definition.com/transfer-learning-for-dog-breed-identification/] To fine-tune the model all convolutional Xception layers are frozen. [https://www.depends-on-the-definition.com/transfer-learning-for-dog-breed-identification/] The model is compiled and trained on the kaggle dataset. Then the transfer learning technique is applied again to compile and train the model on mixed dog breed dataset.
\par To predict a dog breed with the model the bottleneck features from the Xception model and the best weight file is compiled and used to make a predicted vector for an image. The max of that vector is used to determine the dog breed by using that number and going to that index in the list of unique labels(contains·¶ dog breeds).[https://github.com/srikantrao/Dog-Breed-Classifier/blob/master/README.md]
\par \par What projects influenced our model the most? 

\section* {Training}
\indent \par
\par The team decided to get a model correctly predicting the pure breed dogs first before moving on to mixed breeds. This model was trained on a data set from Kaggle that contained 120 different dog breeds. First a CNN was trained to see how well it could do at predicting the dog breed but with little success. A model using transfer learning was then trained with the same dataset. The transfer learning model was able to accurately predict purebred dog breeds. The same technique was applied to the mixed dog breed dataset. At first all convolutional layers of the Xception model were frozen and the other layers were re-trained with the mixed dog breed dataset. This model was only able to accurately predict one of the parents. The team tried re-training more layers of the Xception model and training for more epochs because the mixed dog breed dataset has less data. 


	
	\begin{figure}[]
\centering 
\begin{subfigure}{.5\textwidth}
\centering

\label{}
\end{subfigure}%

\caption{\label{}}
\end{figure}

\section* {Conclusion}
\indent \indent \par Testing
\par What results? How did they improve with changes? The first results were…
\par We improved by..
\par A reflection on the successes and failures of your attempt:  
\par How did it go?
	

\section* {Future Work}

\par	How we can improve our results, what more can be done, etc. 


 

\begin{bibdiv}
\begin{biblist}
\bib{Climate}{misc}{title={Actuaries Climate Index Development and Design},
author={Climate Change Committee}, author={Climate Index Working Group},author={The American Academy of Actuaries},author={The Casualty Actuarial Society},author={The Canadian Institute of Actuaries},author={The Society of Actuaries},
date={2016},
note={Web. 28 Mar. 2017.}}

\bib{ClimateImpact}{webpage}{author={Environmental Protection Agency}, title={Climate Impact on Costal Areas}, 
date={06 Oct. 2016},
note={Web. 28 Mar. 2017.}}


\bib{Appraisal}{report}{author={Kolk, Stephen Lee}, title={An Appraisal of the Actuaries' Climate Risk Index},
date={May 18, 2016},
subtitle={The Economics Impacts of Sea-Level Rise in Hampton Roads},
note={Paper 7.}}


\bib{Drives}{article}{author={Milne, Glenn}, title={How the climate drives sea-level changes},
note={A \& G 2008; 49 (2): 2.24-2.28.doi: 10.1111/j.1468-4004.2008.49224.x}}



	
	
	




\end{biblist}
\end{bibdiv}
\end{document}
